{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-postprocessor-cohere-rerank\n",
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import shutil\n",
    "import logging\n",
    "import sys\n",
    "import chromadb\n",
    "import openai\n",
    "import time\n",
    "import nltk\n",
    "import nest_asyncio\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    "    SummaryExtractor)\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from sherpa_reader import LLMSherapaReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from document_sorter import DocumentSorter\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for ChromaDB storage\n",
    "PERSIST_DIR = \"./chromadb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Previous DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(PERSIST_DIR):\n",
    "        shutil.rmtree(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiate ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"class_materials2\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading Data\")\n",
    "documents = SimpleDirectoryReader(\"data\", file_extractor={\".pdf\" : LLMSherapaReader()}).load_data()\n",
    "info, questions, garbage, broken = DocumentSorter().sort(documents)\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dad Loading & Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data through the pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "transformations=[\n",
    "    SentenceWindowNodeParser.from_defaults(\n",
    "        # how many sentences on either side to capture\n",
    "        window_size=3,\n",
    "        # the metadata key that holds the window of surrounding sentences\n",
    "        window_metadata_key=\"window\",\n",
    "        # the metadata key that holds the original sentence\n",
    "        original_text_metadata_key=\"original_sentence\",\n",
    "    ),\n",
    "    #SummaryExtractor(summaries=[\"prev\", \"self\", \"next\"], llm=llm),\n",
    "    #KeywordExtractor(keywords=3, llm=llm),\n",
    "    OpenAIEmbedding(model_name=\"text-embedding-3-large\")\n",
    "],\n",
    "vector_store=vector_store\n",
    ")\n",
    "\n",
    "nodes_post_pipe = pipeline.run(documents=info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nodes_post_pipe.txt\", \"w\") as file:\n",
    "    for node in nodes_post_pipe:\n",
    "        file.write(node.text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-large\"), storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_post_processor = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n",
    "cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=3)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    llm=llm,\n",
    "    node_postprocessors=[\n",
    "        window_post_processor,\n",
    "        cohere_rerank\n",
    "    ],\n",
    "    )\n",
    "response = query_engine.query(\"what information can you tell me about the textbook?\")\n",
    "display_response(\n",
    "    response=response, source_length=1000, show_source=True, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes_post_pipe, llm=llm, num_questions_per_chunk=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_score = sum(result.metric_dict[\"mrr\"].score for result in eval_results) / len(eval_results)\n",
    "print(f\"mrr_score: {mrr_score}\")\n",
    "\n",
    "hit_rate_score = sum(result.metric_dict[\"hit_rate\"].score for result in eval_results) / len(eval_results)\n",
    "print(f\"hit_rate_score: {hit_rate_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "evaluator_gpt4 = FaithfulnessEvaluator(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "\n",
    "question_generator = DatasetGenerator.from_documents(info)\n",
    "eval_questions = question_generator.generate_questions_from_nodes(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "def evaluate_query_engine(query_engine, questions):\n",
    "    c = [query_engine.aquery(q) for q in questions]\n",
    "    results = asyncio.run(asyncio.gather(*c))\n",
    "    print(\"finished query\")\n",
    "\n",
    "    total_correct = 0\n",
    "    for r in results:\n",
    "        # evaluate with gpt 4\n",
    "        eval_result = (\n",
    "            1 if evaluator_gpt4.evaluate_response(response=r).passing else 0\n",
    "        )\n",
    "        total_correct += eval_result\n",
    "\n",
    "    return total_correct, len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = evaluate_query_engine(query_engine, eval_questions[:30])\n",
    "\n",
    "print(f\"score: {correct}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "\n",
    "queries = list(qa_dataset.queries.values())[:10]\n",
    "\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator()\n",
    "relevancy_evaluator = RelevancyEvaluator()\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "{\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "workers=8,\n",
    ")\n",
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine, queries=queries\n",
    ")\n",
    "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "print(f\"faithfulness_score: {faithfulness_score}\")\n",
    "\n",
    "relevancy_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['relevancy'])\n",
    "print(f\"relevancy_score: {relevancy_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
